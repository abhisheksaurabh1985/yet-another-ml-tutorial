{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `logit`\n",
    "\n",
    "- __[This](https://stackoverflow.com/questions/34240703/whats-the-difference-between-softmax-and-softmax-cross-entropy-with-logits)__ link on StackOverFlow has a much better explanation.\n",
    "- Logit is a function that maps probabilities `[0, 1]` to `[-inf, +inf]`.\n",
    "\n",
    "$L = ln(p/(1-p))$ and $p = 1/ (1 + exp(-L))$\n",
    "\n",
    "Probability of 0.5 corresponds to a logit of 0. Negative logit correspond to probabilities less than 0.5, positive to > 0.5.\n",
    "\n",
    "- Softmax is a function that maps `[-inf, +inf]` to `[0, 1]` similar as Sigmoid. But Softmax also normalizes the sum of the values(output vector) to be 1.\n",
    "\n",
    "- Tensorflow \"with logit\": It means that you are applying a softmax function to logit numbers to normalize it. The input_vector/logit is not normalized and can scale from `[-inf, inf]`.\n",
    "\n",
    "This normalization is used for multiclass classification problems. And for multilabel classification problems sigmoid normalization is used i.e. tf.nn.sigmoid_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Precision`, `Recall` and `F1-Score`\n",
    "\n",
    "1. __Precision__: Ratio of correctly predicted positive observations to the total predicted positive observations. \n",
    "$$ Precision = TP/ (TP+FP) $$\n",
    "\n",
    "2. __Recall (Sensitivity)__ : Ratio of correctly predicted positive observations to all the observations in the actual positive class.\n",
    "$$ Recall = TP/ (TP+FN) $$\n",
    "\n",
    "3. __F1 Score__: Weighted average of precision and recall. This score takes both FPs and FNs into account. Intuitively, it is not as easy to understand as accuracy, but F1 is more useful than accuracy, especially if we have an unenven class distribution. Accuracy works best iif both FPs and FNs have similar cost. If the cost of FPs and FNs are very different, it's better to look at both Precision and Recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
